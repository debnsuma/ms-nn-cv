{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxkggtFcSAQB"
   },
   "source": [
    "##Agenda\n",
    "\n",
    "- In this lecture we will explain how forward & backward propagation works in CNN from scratch using numpy and scipy(for mathematical operations) to classify Images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir-skYXLXU3_"
   },
   "source": [
    "### About MNIST dataset:\n",
    "\n",
    "\n",
    "- MNIST is a dataset consisting of 70,000 black-and-white images of handwritten digits. \n",
    "- Each image is 28 x 28 (= 784) pixels. \n",
    "- Total of 10 classes (numbers from 0 to 9)\n",
    "- Each pixel is encoded as an integer from 1 (white) to 255 (black): \n",
    "  - the higher this value the darker the color.   \n",
    "  - 60,000 images are used as train set and 10,000 as test set. \n",
    "\n",
    "\n",
    "  <center>  <img src='https://drive.google.com/uc?id=1zLPt6sTQ1HGn3q1jywjOrJs-73QO2BYY' height=230></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEgAy2096XSs"
   },
   "outputs": [],
   "source": [
    "#Importing required Modules\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b59NkidARcYD"
   },
   "source": [
    "## Overview of CNN Model :\n",
    "   - That we are going to build from Scratch\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1Tttc9HtqZwfHsFYWDUdhH0pUpxp7J_AK' height=500></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRl3Hz_oAZpr"
   },
   "source": [
    "Defining our Base class Layer to specify the default layer properties that will be inherited to our Convolutional, Reshape, MaxPool, activation classes which will be derived from the Layer class.\n",
    "<br> The Layer class has two attributes:\n",
    "* forward : which takes in the input to the layer and returns the output via specified calculation for each layer\n",
    "* backward : which takes in the output gradient to update the layer's parameters and return the input gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItIAdJbV6i_o"
   },
   "outputs": [],
   "source": [
    "#Base layer class to specify the Layer properites for our Reshape and Convolution Layer to inherit\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: return output\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: update parameters and return input gradient\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXpaTdkCZe-T"
   },
   "source": [
    "## Forward propagation in CNN \n",
    "\n",
    "- Forward propagation is simply feeding the data in the forward direction to the model or neural network.\n",
    "- Like in our case, each of our training images is processed through the-\n",
    "  - convolution, activation, pooling, reshape, Dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsR3F7hyWY-0"
   },
   "source": [
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "- The convolutional class inherits from our base Layer class and takes input parameters   \n",
    " - input_shape, \n",
    " - kernel_size and \n",
    " - depth/numer of kernels/filters) for our layer. \n",
    "\n",
    "\n",
    "Defining & Initializaing the input & output shapes of input_image, kernel_size & bias:\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ14TZP6cfHY"
   },
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        #Input_shape is 3 dimensional (dxhxw), input depth representing no. of image input channels, input_height = image height and input_width = image width\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        #Depth represents the number of kernels of our convolutional layer\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        #Number of channels in the image, i.e. 3 in a RGB image, 2 in a grayscale image\n",
    "        self.input_depth = input_depth\n",
    "        #Calculating our conv. layer output of 3 dimensions, first dim = number of filters/kernels, \n",
    "        #second dim = height of the output matrix after applying convolution i.e input image height - kernel size + 1 by rule\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        #Kernels shape specifies the shape of the kernels produced, 4 dimensions depth = no. of kernels, input_depth = image channels, kernel_size = kernel dimension \n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        #Initalizing the Kernels weights randomly\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        #Initializing the biases randomly\n",
    "        self.biases = np.random.rand(*self.output_shape)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #TODO\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q6ptt5ZZAvT"
   },
   "source": [
    "\n",
    "- After taking in these parameters, the Convolutional layer processes the inputs through our forward and backward functions while training our model.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZBgfBOzpQhy"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Forward propagation in Convolution layer?\n",
    "\n",
    "- The forward function iterates through every filter in every image channel and\n",
    " - it pass over a kernel of specified size over the image and \n",
    " - perform the convolution operation between the kernels and the input image.\n",
    "\n",
    " <center>  <img src='https://drive.google.com/uc?id=1PPy9sjvh36qktkqeFnB-WBiNj3ZPNufU' width=500 height=190></center>\n",
    "- Forward propagation mathematically for the *Convolutional Layer* is as follows:\n",
    "\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1vMgkDBw6pRwdLA7J-gZ_mFG9Frytkd2Z' width=500 height=80></center>\n",
    "\n",
    "- where Y is the output of our convolutional operation which is equal to the Bias plus the sum of cross-correlations between the input image (Xᵢ) and our convolution kernel/filter (Kᵢⱼ), represented by * .\n",
    "- This equation is implemented to calculate output through every step of our convolutional operation till every image is applied the convolutional operation for the depth / number of kernels of the layer  \n",
    "   \n",
    "  - Expanding the equation, we get:\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1fsFbtbIIIGfBY_3fD9itb_fwD3qGE76z' width=500 height=200 width=150></center>\n",
    "where d is the depth/number of filters and n is the input depth.\n",
    "<br>\n",
    "\n",
    "\n",
    "- By using these equations, we pass the filters with their weights over the image matrix, to compute the output of the convolutional layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38O8juznauyc"
   },
   "source": [
    "Here we perform the convolution operation using correlate2d() method of scipy library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGs_aXSGcwqX"
   },
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        #Input_shape is 3 dimensional (dxhxw), input depth representing no. of image input channels, input_height = image height and input_width = image width\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        #Depth represents the number of kernels of our convolutional layer\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        #Number of channels in the image, i.e. 3 in a RGB image, 2 in a grayscale image\n",
    "        self.input_depth = input_depth\n",
    "        #Calculating our conv. layer output of 3 dimensions, first dim = number of filters/kernels, \n",
    "        #second dim = height of the output matrix after applying convolution i.e input image height - kernel size + 1 by rule\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        #Kernels shape specifies the shape of the kernels produced, 4 dimensions depth = no. of kernels, input_depth = image channels, kernel_size = kernel dimension \n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        #Initalizing the Kernels weights randomly\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        #Initializing the biases randomly\n",
    "        self.biases = np.random.rand(*self.output_shape)\n",
    "\n",
    "    #Forward pass, takes input and computes the output by applying the above convolution \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        #Inititialize output matrix with output_shape \n",
    "        self.output = np.zeros(self.output_shape)\n",
    "\n",
    "        #Two nested for loops for first traversing all filters (depth), then all channels (input_depth) in every input image\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "               #Output is calculated by adding the biases of the layer with the Cross Correlation \n",
    "               #between image and the kernel,valid stands for no padding \n",
    "                self.output[i] = self.biases[i]+ signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXw3bd3Mo2ew"
   },
   "source": [
    "\n",
    "#### How Backpropagation works in Convolution ?\n",
    "\n",
    "Recall what is backpropagation ?\n",
    "- Backward Propagation refers to updating the weights and biases of our model with respect to the loss of our model.\n",
    "\n",
    "####In case of Convolution:\n",
    "\n",
    "- The parameters that we need to update -\n",
    "    -  kernels ,inputs and biases\n",
    "- For updating the kernels and biases we need to compute their gradients, \n",
    "- From the forwardpropagation we have the derivative of the error from which we will compute:\n",
    "\n",
    "  - Derivative of L (error of the network) with respect to kernels (K) and biases of the layer (B)\n",
    "  - Derivative of L with respect to the input (X)\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1kqs8v_XjGAhKw26TxLyyBloHy_x-bKVt' height=350 width=575></center>\n",
    "\n",
    "\n",
    "#### 1. Kernel updates: \n",
    "\n",
    "- The Image below depicts the computation of the derivative of the Loss (L) with respect to the Kernels (K)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1oFv2vJeWr2_nhzVjitfs7OyQHkoCAeJr' height=350></center>\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1XI9J7oo0a6lHHFgRumOst_WZZ6QNB7Yz' height=350 width=575></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Bias updates:\n",
    "\n",
    "Then we Calculate the derivative of L (error of the network) with respect to the bias of the layer as follows:\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1Wj-yxhihBkzr2qk5QMojiApWY7JnWw9a' height=350 width=575></center>\n",
    "\n",
    "\n",
    "#### 3. Input updates:\n",
    "\n",
    "- The derivative of L (error of the network) with respect to the Input of the layer as follows:\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1iGEPp7nc1eTHY58CjVe1bzFgZBi5GzfI' height=350 width=575></center>\n",
    "\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1M3CzI9KnCgJG-1_RfJK9Ya15PqH8iLDI' height=350 width=575></center>\n",
    "\n",
    "<br><br>Now let's understand how this works<br>\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=11VhVh2zqmHXIkaUP-sZ3Bi9fryNd__CU' height=500></center>\n",
    "\n",
    "- This depicts that the input gradient i.e. derivative of Loss w.r.t. to the input is equal to the output gradient i.e. derivative of Loss w.r.t. the output gradient fully cross-correlated (* symbol) with the 180 degress rotated kernel.\n",
    "\n",
    "- This concludes that the input gradient is equal to the output gradient fully convolved with the kernel\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfVriaf3dzhK"
   },
   "source": [
    "**Quiz 1** : \n",
    "\n",
    "  1.  **How can we calculate the gradients to update Kernel weights in Convolutional Layer during Backpropagation ?**<br>\n",
    "\n",
    "    A) Calculating derivative of image pixels with respect to kernel weights<br>\n",
    "    B) Calculating derivative of loss with respect to kernel weights <br>\n",
    "    C) Calculating the derivative of kernels with respect to bias<br>\n",
    "    D) Calculating the derivative of input with respect to loss<br>\n",
    "<br>\n",
    "\n",
    "**Solution** : \n",
    "\n",
    "B)\n",
    "\n",
    "- We calculate the gradients of the loss function by computing the derivative (Loss/output) with the help of the derivative (Loss/Kernels) \n",
    "$\\frac{\\partial Loss}{\\partial {w_i}}$, derivative (Loss/Bias) $\\frac{\\partial Loss}{\\partial {b_i}}$and derivative (Loss/Input)$\\frac{\\partial Loss}{\\partial {X_i}}$\n",
    "\n",
    "- We update the kernels and the biases of our convolutional layer by subtracting the kernels gradients and output gradient w.r.t. the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVOQ8Lpe6qIF"
   },
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        #Input_shape is 3 dimensional (dxhxw), input depth representing no. of image input channels, input_height = image height and input_width = image width\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        #Depth represents the number of kernels of our convolutional layer\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        #Number of channels in the image, i.e. 3 in a RGB image, 2 in a grayscale image\n",
    "        self.input_depth = input_depth\n",
    "        #Calculating our conv. layer output of 3 dimensions, first dim = number of filters/kernels, \n",
    "        #second dim = height of the output matrix after applying convolution i.e input image height - kernel size + 1 by rule\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        #Kernels shape specifies the shape of the kernels produced, 4 dimensions depth = no. of kernels, input_depth = image channels, kernel_size = kernel dimension \n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        #Initalizing the Kernels weights randomly\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        #Initializing the biases randomly\n",
    "        self.biases = np.random.rand(*self.output_shape)\n",
    "\n",
    "    #Forward pass, takes input and computes the output by applying the above convolution \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        #Inititialize output matrix with output_shape \n",
    "        self.output = np.zeros(self.output_shape)\n",
    "\n",
    "        #Two nested for loops for first traversing all filters (depth), then all channels (input_depth) in every input image\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                #Output is calculated by adding the biases of the layer with the Cross Correlation between image and the kernel, valid stands for no padding in our correlation calculation inputs\n",
    "                self.output[i] = self.biases[i]+ signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #Intializing the gradient of the kernels as zeros\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        #Intializing the gradient of the input as zeros\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        #Nested for loop for updating the gradients first traversing all filters (depth), then all channels (input_depth) in every input image to update the gradients of kernels and inputs\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                #Calculate kernels gradient in every i and j index in the kernel, compute correlation between image and output graident\n",
    "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
    "                #Calculate input gradient by sliding the kernel on the output gradient matrix\n",
    "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
    "\n",
    "        #Update the kernels and biases w.r.t. learned features (stored in gradients)\n",
    "        #Gradients are multiplied with the learning rate to update the kernels and biases  \n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * np.sum(output_gradient)\n",
    "        \n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJgFS24bfATw"
   },
   "source": [
    "### Activation Functions:\n",
    "\n",
    "We have used majorly 3 activation functions (ReLU, TanH and Softmax) in our CNN model architecture which are explained below:\n",
    "\n",
    "In **Backpropagation** of all activations, we apply the backward function of the activation on the input.\n",
    "\n",
    "=> The Gradients of the previous layer are multiplied with the Layer Input on which we apply the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukfb6Sf36yDx"
   },
   "outputs": [],
   "source": [
    "#Base Activation class to specify the default properties of the Activation Layer from which we will derive our Activation functions ReLU, Softmax, TanH\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        #Calculation for the activation function\n",
    "        self.activation = activation\n",
    "        #Calculation for derivative of activation function which will be handy while backpropagation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #Implement the backward function for the activation, by multiplying the output gradient and the derivative of the loss\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjfpsnlRhSJM"
   },
   "source": [
    "#### ReLU\n",
    "\n",
    "The ReLU activation activation works on a simple principle. \n",
    "\n",
    "- If the input value is less than zero, the output is zero\n",
    "\n",
    "- else if the input value is greater than or equal to zero, the output is same is as input value\n",
    "<br>\n",
    "\n",
    " ReLU introduces non-linearity to our CNN and helps in over-reliability of the network on some features as it drops neurons non-linearly.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1arzm7HH9_9TvOgPihZniQnaRwWgaKJfF' width=220 height=80></center><br>\n",
    "\n",
    "During **Backpropagation**, the ReLU activation works as follows :<br> <br>\n",
    "The ReLU function is defined as: <br>\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1HyqadM9ED4XhVt-B-HGMIom8VHlOybYR' height=270></center><br>\n",
    "\n",
    "For $x > 0$ the output is $x$, i.e. $f(x) = max(0, x)$\n",
    "\n",
    "Derivative of $f(x)$ = $f'(x)$ : if $x < 0$, output is $0$. if $x > 0$, output is 1.\n",
    "\n",
    "The derivative $f '(0)$ is not defined. So it's usually set to $0$.<br>\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1kxyOE1NIfVDrZeD35fww8FxNulJbrqsR' width=220 height=80></center><br>\n",
    "\n",
    "\n",
    "Code representation of the above calculation can be represented as :\n",
    "<pre>def relu_prime(x):\n",
    "   return np.where(x <= 0, 0, 1)</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2j7dWOUmhHtE"
   },
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "  def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.where(x > 0, x, 0)\n",
    "\n",
    "        def relu_prime(x):\n",
    "            return np.where(x <= 0, 0, 1)\n",
    "\n",
    "        super().__init__(relu, relu_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BS-wID4iuXf"
   },
   "source": [
    "#### TanH\n",
    "\n",
    "The TanH activation function takes any input value and outputs the value in the range -1 to 1\n",
    "\n",
    "\n",
    "- The greater the input value, the closer the output value will be equal to 1\n",
    "\n",
    "\n",
    "- The smaller the input, the closer the output value will be to -1\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1MqSgrbLwMz7u3G7LowITwHDMVHeDaKok' height=240></center>\n",
    "\n",
    "By practice, the TanH activation function is used when we want our modle to converge quickly with a greater value of learning rate which results in higher values of gradients during training, therefore making our model converge faster with a good accuracy roven over time.\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1eCuIjpRCGlldmP98l1Ubk1nCXiTlquFv' height=110 width=200></center>\n",
    "\n",
    "\n",
    "In **Backpropagation** of TanH activation, we calculate the gradients for tanh by computing the derivative of TanH function.\n",
    "\n",
    "Calculating the gradient for the tanh function also uses the quotient rule:\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1tJNVaouljjOEykAD4L9aWFwsE7YNrdaf' height=240></center>\n",
    "\n",
    "The derivative of tan(x) is therefore, `1 - np.tanh(x) ** 2`, which we have implemented using np.tanh in the code cell below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwYKpkIaiuXf"
   },
   "outputs": [],
   "source": [
    "class TanH(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanh_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GktonBbHiuXf"
   },
   "source": [
    "#### Softmax\n",
    "\n",
    "the Softmax activation function is generally used as last layer in a neural network\n",
    "\n",
    "\n",
    "\n",
    "- softmax converts the Dense's layer (classifcation layer) outputs to a probability map such that the sum of the outputs is equal to 1\n",
    "\n",
    "\n",
    "\n",
    "- Softmax is used in classfication problems, like in our case where the number of classes is 2\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=17s1wV-Jl2hFMGmOIZ0Ri38Znp0Jo8aiy' height=160></center>\n",
    "\n",
    "\n",
    "**Backpropagation** \n",
    "\n",
    "During the backward pass, a softmax layer receives a gradient, the partial derivative of the loss with respect to its output values.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "To use the softmax function in neural networks, we need to compute its derivative. If we define $\\Sigma_C = \\sum_{d=1}^C e^{z_d} \\, \\text{for} \\; c = 1 \\cdots C$ so that $y_c = e^{z_c} / \\Sigma_C$, then this derivative ${\\partial y_i}/{\\partial z_j}$ of the output $\\mathbf{y}$ of the softmax function with respect to its input $\\mathbf{z}$ can be calculated as:\n",
    "<br><br>\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1hXYwgq4FyXnYK0rwG7o4FjpRYELUNZUM' height=90></center>\n",
    "\n",
    "The first part of this expression propagates the softmax values down the diagonal, and then we calculate the product of softmax values with respect to input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msItpFfziuXf"
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        n = np.size(self.output)\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhmRILi6azl1"
   },
   "source": [
    "\n",
    "### MaxPooling Layer\n",
    "\n",
    "The MaxPool layer selects the maximum pixel values for every image in a particular kernel area.\n",
    "<br>\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1Wv3mBlveedhmRROXun8Bd8hZVZJ5RqLo' width=400></center>\n",
    "\n",
    "Forward propagation:\n",
    "\n",
    "- We Select the maximum value out of a given kernel area in an Input Image, the filter slides over our image w.r.t. given stride.\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1RliqXph1H-XNH0chWnSqZJLwqZiIQPmO' width=400></center>\n",
    "\n",
    "Backward propagation:\n",
    "\n",
    "- For the backward propagation, we have to select only the elements in our input matrix that were selected during the forward propagation of the maxpool layer, zeroing out all the other indices that were not selected during forward propagation.\n",
    "\n",
    "- In other words the gradient with respect to the input of the max pooling layer will be a matrix of zeros except on the places that was selected during the forward propagation.\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1D27c4xmPb7nHF5_KDXY1GveeoOs9Yy1y' width=400>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb4ReGBjxTfi"
   },
   "source": [
    "**Quiz 2** :\n",
    "\n",
    "\n",
    "**Which of the following is true about MaxPooling Layer** ?\n",
    "\n",
    "A) MaxPooling layer does not have any trainable parameters<br>\n",
    "B) MaxPooling layer helps in reducing computation<br>\n",
    "C) In MaxPooling we take the highest pixel value inside the kernel window<br>\n",
    "D) All of the above\n",
    "\n",
    "Solution :<br>\n",
    "D) \n",
    "\n",
    "- MaxPooling and AveragePooling layers does not perform any learning and are only used for down-sampling the data\n",
    "- Since max pooling is reducing the resolution of the given output of a convolutional layer, it reduces the amount of parameters in the network and consequently reduces computational load.\n",
    "- Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdJICNZP8rji"
   },
   "outputs": [],
   "source": [
    "class MaxPool(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth, stride):\n",
    "        #Input_shape is 3 dimensional (dxhxw), input depth representing no. of image input channels, input_height = image height and input_width = image width\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        #Specifying the shape of input\n",
    "        self.input_shape = input_shape\n",
    "        #Specfiying the kernel size of our MaxPool operation\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        #Specifying the depth/no. of filters\n",
    "        self.depth = depth\n",
    "        #Specifying the depth/channels of our input\n",
    "        self.input_depth = input_depth\n",
    "        #initializing the kernels with random values of shape (kernels_shape)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.stride = stride\n",
    "        self.input_height, self.input_width = input_height, input_width\n",
    "\n",
    "    #forward method to perform the MaxPool operation on the input \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        KH = 1 + (self.input_height - self.kernel_size) // self.stride\n",
    "        KW = 1 + (self.input_width - self.kernel_size) // self.stride\n",
    "        self.output = np.zeros((self.input_depth, KH, KW))\n",
    "\n",
    "        for depth in range(self.input_depth):\n",
    "            for r in range(0, self.input_height-1, self.stride):\n",
    "                for c in range(0, self.input_width-1, self.stride):\n",
    "                    self.output[depth, r//self.stride, c//self.stride] = np.max(self.input[depth, r:r+self.kernel_size, c:c+self.kernel_size])\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate): \n",
    "        self.output_gradient = np.zeros(self.input_shape)\n",
    "        # Intialize gradient of our layer\n",
    "        dx = np.zeros(self.input_shape)\n",
    "        # nested for loops for first traversing all filters (depth), then traversing the height and width of the image\n",
    "        for depth in range(self.input_depth):\n",
    "            for r in range(0, self.input_height-1, self.stride):\n",
    "                for c in range(0, self.input_width-1, self.stride):\n",
    "                    grad_pool = self.output[depth, r*self.stride:r*self.stride+self.kernel_size, c*self.stride:c*self.stride+self.kernel_size]\n",
    "                    mask = (grad_pool == np.max(grad_pool))\n",
    "                    dx[depth, r*self.stride:r*self.stride+self.kernel_size, c*self.stride : c*self.stride+self.kernel_size] = mask*self.output_gradient[depth, r, c]\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HE8mwpIrVpG"
   },
   "source": [
    "### Reshape Layer or Flattening\n",
    "\n",
    "- To reshape arrays inorder to pass through dense layers\n",
    "- The Reshape layer is used to reshape the dimenions of an input array to a desired compatible shape using the numpy.reshape function </br> \n",
    "- For example we may reshape an array of size (5, 4, 1) to a size of (20, 1) using numpy.reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6Jw_I0I6MfM"
   },
   "outputs": [],
   "source": [
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "    #Specify input shape and out shape in the constructor\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    #Forward reshapes the input to the output shape\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    #Backward reshapes the output to the input shape\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYt0ELtehbd_"
   },
   "source": [
    "\n",
    "### Dense Layer\n",
    "\n",
    "### Recall from NN class ?\n",
    "\n",
    "- The Dense layer, also known as fully-connected or linear layer does a simple job of converting the learned features from the images by the convolutional layer TO a feature vector which is used to clasify image based on output form convolutional layers.\n",
    "\n",
    "\n",
    "- W=Weight matrix, X=Input Matrix, b=Bias, Z=Output matrix\n",
    "<center> $Z = W.X + B$</center>\n",
    "\n",
    "- Dense Layer is usually the \"classifciation layer\" as it classfifes the feature vector that we have curated from the features that our Convolutional layer learned to classify into 2 classes \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Recall from Neural Networks class, during **Backpropagation** in Dense Layer, we compute its Gradients using the following process :\n",
    "\n",
    "- Calculate weights gradient by dot product of output gradient and transpose of input\n",
    "- Calculating the input gradient by performing dot product of weights transpose and output gradient\n",
    "- Updating the weights of the layer with weights gradient w.r.t. the rate = learning rate\n",
    "- Updating the bias of the layer with output gradient w.r.t. rate = learning rate\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1o_xBpQe88krbGs4Ja0czyGjSSO0VVrAI' height=200></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQWT8WsV7cu0"
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        #Defining our weights matrix shape\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        #Defining our bias matrix size\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        #Implementing the equation above by summing bias of the dense layer \n",
    "        #with the dot product of the weights and the inputs\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        #Calculate weights gradient by dot product of output gradient and transpose of input\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        #Calculating the input gradient by performing dot product of weights transpose and output gradient\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        #Updating the weights of the layer with weights gradient w.r.t. the rate = learning rate\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        #Updating the bias of the layer with output gradient w.r.t. rate = learning rate\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3umC68z1txJ"
   },
   "source": [
    "**Quiz 3** : \n",
    "\n",
    "**Suppose we have a neuron with weights [-3, 2.8, -1] and bias = 0.5 input [4, 2, 2] \n",
    "<br>What will be the output, when the input [4, 2, 2] is passed to a Dense Layer and its activation function is ReLU ?**\n",
    "<br><br>\n",
    "A) -1.9<br>\n",
    "B) 7.9<br>\n",
    "C) 0<br>\n",
    "D) None of the above<br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Solution** : \n",
    "\n",
    "C)\n",
    "\n",
    "Given $W_1 = -3$, $W_2 = 2.8$ and $W_3 = -1$, <br>\n",
    "$X_1 = 4$, $X_2 = 2$ and $X_3 = 2$ and $b = 0.5$\n",
    "\n",
    "To compute the output of a Dense layer, we have the formula $W^T.X + b$ <br>\n",
    "which translates to (($W_1^T.X_1$) + ($W_2^T.X_2$) + ($W_3^T.X_3)) +  b$ <br><br>\n",
    "Equating value of W, X and b in our eqn. we get :     \n",
    "= ((-3).(4) + (2.8).(2) + (-1).(2)) + 0.5<br>\n",
    "= (-12 + 5.6 - 2) + 0.5<br>\n",
    "= -8.4 + 0.5<br>\n",
    "= -7.9\n",
    "ReLU deactivates (output = 0) the inputs having values < 1,\n",
    "Therefore, the answer is 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfTAiRhbFb8_"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Defining the Log Loss\n",
    "\n",
    "\n",
    "\n",
    "Log-loss is the negative mean of the log of correctly predicted probabilities for each step.\n",
    "\n",
    "Log-loss is depicts how close the prediction probability is to the corresponding true value. The more the predicted probability diverges from the actual value, the higher is the log-loss value.\n",
    "<br>\n",
    "<center>  <img src='https://drive.google.com/uc?id=1aSE-x2hoiNzY0MY00HrfvYLUkSPqgCKc' width=500 height=90></center>\n",
    "\n",
    "We calculate log loss by taking the mean of the log of the true predicted probabilties for each step in every epoch.\n",
    "<br>\n",
    "- During **backpropagating** through our network, we need to calculate the gradients of our network to update them. \n",
    "- For Calculating the gradients of our network, we use the derivative of log loss with respect to the output, to calculate the loss between the output and the actual labels.\n",
    "<center>  <img src='https://drive.google.com/uc?id=1Hh5dnx58DrM1iILiYpQEWClIH7Q7_Qrq'  height=300></center>\n",
    "\n",
    "the derivative of log loss is $1/N *((1 - ytrue) / (1 - ypred) - ytrue / ypred)$ which we have coded as :\n",
    "\n",
    "`(1 - y_pred) - y_true / y_pred) / np.size(y_true)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19gBMiqX6e8q"
   },
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def log_loss_prime(y_true, y_pred):\n",
    "    #log_loss_prime works the same way as log loss \n",
    "    #but is the derivative of the above function which will be used to \n",
    "    # compute layers gradients while backpropagation\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5AuSjo50xCU"
   },
   "source": [
    "### Data Loading & Preprocessing:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yj5egUrZmu8I"
   },
   "outputs": [],
   "source": [
    "# load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v07g2GZzhnyx"
   },
   "outputs": [],
   "source": [
    "#Function to preprocess our MNIST data\n",
    "def preprocess_data(x, y, limit): \n",
    "    #For simplicity we select only 10k images from class 0 and 1 from the dataset\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    #Reshape as to keep first dimension, the selected images only(i.e. 10k in our case)\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    #Normalize all pixel values[0-1], \n",
    "    #dividing by 255 because maxiumum possible pixel RGB value can be 255            \n",
    "    x = x.astype(\"float32\") / 255\n",
    "    #One hot encode all the labels \n",
    "    y = np_utils.to_categorical(y)\n",
    "    # print(y.shape)\n",
    "    y = y.reshape(len(y), 2, 1)\n",
    "    # print(y.shape)\n",
    "    return x, y\n",
    "    \n",
    "\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 10000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56Ymp1nyh-fn"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Defining the Network architecture \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>  <img src='https://drive.google.com/uc?id=1J4uLVMCm_zbb7ubGC9NWYQgojjUf3uEm' height=500></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6iU5pFpku9R"
   },
   "source": [
    "We define our network architecture taking regard of inter-dimension compatibility of layers and always ensuring that the output shape of any layer is always the input shape to its next consecutive layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-Ph1C-dlili"
   },
   "outputs": [],
   "source": [
    "#Define network architecture\n",
    "network = [\n",
    "    #input_shape, kernel_size, depth\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    ReLU(),\n",
    "    #input_shape, kernel_size, depth, stride\n",
    "    MaxPool((5,26,26), 2, 5, 1),\n",
    "    #input_shape, output_shape\n",
    "    Reshape((5, 25, 25), (5 * 25 * 25, 1)),\n",
    "    #input_size, output_size\n",
    "    Dense(5 * 25 * 25, 100),\n",
    "    TanH(),\n",
    "    #input_size, output_size\n",
    "    Dense(100, 2),\n",
    "    Softmax()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtdJ0Ed8fNYS"
   },
   "source": [
    "### Training the network\n",
    "\n",
    "For training the network, the train function executes in the range of the epochs and after each epoch,\n",
    "- predicts on every data point during the training process, \n",
    "- compute the log loss and sum it for every iteration and \n",
    "- compute the gradient for each layer in the reverse flow of ournetwork to perform backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lrbbyQE5-4W"
   },
   "outputs": [],
   "source": [
    "def train(network, loss, loss_prime, x_train, y_train, epochs = 10, learning_rate = 0.01):\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            #Forward pass to predict on the training data, to improve the network\n",
    "            output = predict(network, x)\n",
    "\n",
    "            #Summing the losses to optimize the network's weights and biases\n",
    "            error += loss(y, output)\n",
    "\n",
    "            #Perform backward pass through every layer by computing the gradients \n",
    "            #by reversing the network to perform backpropagation\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        print(f\"Epoch : {e + 1}/{epochs}, loss = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "rTukRJtnh8He",
    "outputId": "2ab826da-1fed-4fa9-b91b-df21220018e9"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Fitting our model to the data by calling the train function\n",
    "train(\n",
    "    network,\n",
    "    log_loss,\n",
    "    log_loss_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs = 10,\n",
    "    learning_rate = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDAuHkbzoWmX"
   },
   "source": [
    "### Evaluating on Test data\n",
    "- The predict function takes an image input and then performs forward propagation through every layer in our network inorder to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMG8kQpKn-Je"
   },
   "outputs": [],
   "source": [
    "#Function to make a prediction using our neural network on a given input\n",
    "def predict(network, input):\n",
    "    output = input\n",
    "    #Performing forward pass to our network through every consecutive layer\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFL2hLW4pIEm"
   },
   "source": [
    "Computing the accuracy of our trained network on unseen data(x_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG7UDjNm20ex",
    "outputId": "1238baf3-2cf4-418c-8a47-c762edaeb609"
   },
   "outputs": [],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrTHT7hMIARa",
    "outputId": "b48dab99-a009-40a7-e98d-3b53e45e67a4"
   },
   "outputs": [],
   "source": [
    "#Function to calculate Accuracy of our network on testing data\n",
    "correct = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    #Checking if the predicted label is equal to the true label\n",
    "    #Since our output is an array of class probabilities,\n",
    "    #we select the maximum value index by using the function 'numpy.argmax()'\n",
    "    #which gives us the class label\n",
    "\n",
    "    if np.equal(np.argmax(output), np.argmax(y)):\n",
    "       correct += 1\n",
    "\n",
    "print(f\"Accuracy of the Network on Test data is {(correct/2115) * 100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d4evWiOr5sX"
   },
   "source": [
    "### Random Sample testing\n",
    "- To evaluate our model, we must visualize the outputs and predictions to get a better understanding of the data and model performance.\n",
    "- The test_random_sample function genreates a random number in the range 0 to 10000 (size of our test data),\n",
    " -  then indexes the test data on the random number to fetch a random image which is of the dimensions 1x28x28, we drop the first dimension to make the image comatible to plot with plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w_nu0ilcEgo"
   },
   "outputs": [],
   "source": [
    "#Test the network on a random sample from the mnist test data\n",
    "def test_random_sample():    \n",
    "    sample_index = random.randint(0, 100)\n",
    "    x_sample, y_sample = x_test[sample_index], y_test[sample_index]\n",
    "    output = predict(network, x_sample)\n",
    "    x_sample = np.squeeze(x_sample, axis = 0)\n",
    "    plt.rcParams[\"figure.figsize\"] = [2, 2]\n",
    "    plt.imshow(x_sample)\n",
    "    plt.axis('off')\n",
    "    print(f\"Predicting on image sample {sample_index} from test data\")\n",
    "    print(f\"Prediction : {np.argmax(output)}\")\n",
    "    print(f\"Actual : {np.argmax(y_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hzDNoME_OHr",
    "outputId": "c8c7cf5f-6896-469f-8ee4-0df9588fd3cd"
   },
   "outputs": [],
   "source": [
    "test_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jauqGgJu_b4i",
    "outputId": "ebff437e-bceb-47ad-a0ad-f22091060b5f"
   },
   "outputs": [],
   "source": [
    "test_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLLE8BDD_k-q",
    "outputId": "f0e77f96-4be9-4911-a72a-d71f7275839c"
   },
   "outputs": [],
   "source": [
    "test_random_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nbum13oKTwDs",
    "outputId": "32b1d76e-f9a8-4e36-cd43-4dd78d3c957e"
   },
   "outputs": [],
   "source": [
    "test_random_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-jru1F2imO3"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Visualizing layer outputs\n",
    "\n",
    "\n",
    "- To get a better understanding of what each layer does to the input image and what it's individual output looks like,\n",
    "\n",
    "- we create a function to pass a random test image through our trained network and \n",
    "\n",
    "- plot the image when its passed through the Convolutional, MaxPool and ReLU Layers of our network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlhBFhb7TpMw"
   },
   "outputs": [],
   "source": [
    "def layer_outputs(network, input):\n",
    "    #Initialize the output\n",
    "    output = input\n",
    "    #Intitialize the layer number\n",
    "    layer_n = 0\n",
    "    #Iterate through layers in our network\n",
    "    for layer in network:\n",
    "        #Since we need only the Conv., MaxPool and ReLU layers, we specify layer_number<=3\n",
    "        if layer_n <= 3: \n",
    "            #Plot the image using plt.imshow(), drop the first dimension of our image\n",
    "            #since we do not need for plotting the image\n",
    "            plt.imshow(output[0,:,:]) \n",
    "            #Turning off the axis (not necessary)\n",
    "            plt.axis('off')    \n",
    "        \n",
    "        #Compute the output of the current layer\n",
    "        output = layer.forward(output)\n",
    "        #Incremenet the layer number on every iteration\n",
    "        layer_n = layer_n + 1\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25mNIOoBoQzd"
   },
   "source": [
    "Plot displaying Layer outputs in the order:\n",
    "\n",
    "**Input Image => Convolutional output => ReLU output => MaxPool output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lseoU77_AuAw",
    "outputId": "147acb33-1235-4dbb-c3f4-aeceaa7f3b6f"
   },
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, 2115)\n",
    "x_sample, y_sample = x_test[sample_index], y_test[sample_index]\n",
    "layer_outputs(network, x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbxB4QFXVI2C",
    "outputId": "2ffced55-83bf-49c3-9953-1ead9d714bab"
   },
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, 2115)\n",
    "x_sample, y_sample = x_test[sample_index], y_test[sample_index]\n",
    "layer_outputs(network, x_sample)\n",
    "#Input Image => Convolutional output => ReLU output => MaxPool output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbzL-c0aAeBV",
    "outputId": "2d2c266e-8ade-466d-857c-9582ef2a6f22"
   },
   "outputs": [],
   "source": [
    "sample_index = random.randint(0, 2115)\n",
    "x_sample, y_sample = x_test[sample_index], y_test[sample_index]\n",
    "layer_outputs(network, x_sample)\n",
    "#Input Image => Convolutional output => ReLU output => MaxPool output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TppwmN35lgou"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Layer output dimensions\n",
    "\n",
    "\n",
    "\n",
    "We create a function to visualize how our network changes the dimensions of our input image (1x28x28) when passed through every layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJOf0OpxV9DY"
   },
   "outputs": [],
   "source": [
    "def layer_shape(network, input):\n",
    "    output = input\n",
    "    #Performing forward pass to our network through every consecutive layer\n",
    "    for layer in network:\n",
    "        #Pass the image to every layer's forward method\n",
    "        output = layer.forward(output)\n",
    "        #Print layer name\n",
    "        print(layer)\n",
    "        #Print layer shape\n",
    "        print(output.shape)\n",
    "    \n",
    "    #Locating the index in our output class probabilities with the highest value\n",
    "    #to get the class label\n",
    "    return output.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05XmwROXtNlz"
   },
   "source": [
    "**Input (1x28x28) => Convolution (5 x 3x3 kernels shape) => Output (5, 26, 26) => ReLU => MaxPool (5 x 2x2 kernels shape) => Output (5, 25, 25)=> Reshape (input = 5x25x25) => Output (3125x1) => Dense (input = 3125,1) (output = 100) => TanH => Dense (input = 2, output = 1) => Softmax => predicted class probabilities)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPtW-KZ2qOkD",
    "outputId": "6dd0c867-b403-4f00-c496-d0cb8a5bd8f5"
   },
   "outputs": [],
   "source": [
    "print(np.argmax(y_sample))\n",
    "layer_shape(network, x_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWh-8fCxQYqn"
   },
   "source": [
    "## EXERCISE:\n",
    "- Write the class for sigmoid activation function and define function for forward & backward propagation and retrain the model with sigmoid activation(replace softmax) in the output layer "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c410295bfbef04d36b350637c05ef8df3a08239fed6bc74d3119fe46a0288bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
